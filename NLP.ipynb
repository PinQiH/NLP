{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca467042",
   "metadata": {},
   "source": [
    "https://ithelp.ithome.com.tw/users/20120030/ironman/5515?page=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a22a4",
   "metadata": {},
   "source": [
    "# Day3-Hugging Face 本地端開發環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7acad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") #使用情感分析\n",
    "classifier(\n",
    "    [\n",
    "        \"寶寶覺得苦，但寶寶不說\",\n",
    "        \"我愛寶寶\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f460c21",
   "metadata": {},
   "source": [
    "# Day6-初探 Hugging Face Dataset Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43377f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "ds_builder = load_dataset_builder(\"poem_sentiment\") #用load_dataset_builder 不會把資料下載下來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_builder.info.description)\n",
    "print(ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f27374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "sentiment = load_dataset(\"poem_sentiment\") #下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27067251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#切割資料\n",
    "train_ds = sentiment[\"train\"]\n",
    "valid_ds = sentiment[\"validation\"]\n",
    "test_ds = sentiment[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92985a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 dataset 轉成 Pandas\n",
    "import pandas as pd\n",
    "\n",
    "sentiment.set_format(type=\"pandas\")\n",
    "\n",
    "df = sentiment[\"train\"][:]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97cb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 label 轉成文字\n",
    "def label_int2str(row):\n",
    "\treturn sentiment[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e10de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset 的 label 分佈圖\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label_name\"].value_counts().plot.barh()\n",
    "plt.title(\"Poem Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以把 pandas 處理過的轉成新的 dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "label_name_dataset = Dataset.from_pandas(df)\n",
    "label_name_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle 資料\n",
    "sentiment_train = sentiment[\"train\"].shuffle(seed=5566).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f444ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#用詩句的長度過濾資料\n",
    "sentiment_filtered = sentiment.filter(lambda x: len(x[\"verse_text\"]) > 30)\n",
    "sentiment_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把詩句轉成文字長度\n",
    "new_dataset = sentiment.map(\n",
    "    lambda x: {\"verse_text\": [ len(o) for o in x[\"verse_text\"] ] }, batched=True\n",
    ")\n",
    "new_dataset['test'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34360b5",
   "metadata": {},
   "source": [
    "# Day10-Tokenizer 入門"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efc8ea",
   "metadata": {},
   "source": [
    "###### Character tokenization: 很難讓模型得出有意義的結論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66392ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character tokenization\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = list(string)\n",
    "print(tokenized_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalization\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0fea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把原始句子轉為數字\n",
    "input_ids = [token2idx[token] for token in tokenized_str]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad04c21",
   "metadata": {},
   "source": [
    "###### Word tokenization: 很容易導致參數過大的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65160c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word tokenization\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = string.split()\n",
    "print(tokenized_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2389b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalization\n",
    "token_word2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
    "print(token_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把原始句子轉為數字\n",
    "input_ids = [token_word2idx[token] for token in tokenized_str]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd0021",
   "metadata": {},
   "source": [
    "# Day12-Hugging Face Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c598ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" #直接呼叫transformer model 名字\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #自動使用該 transformer 所使用的 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定 Tokenizer\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc138c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_str = tokenizer(string, padding=True, truncation=True) \n",
    "encoded_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把編碼後的文字還原回來\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_str.input_ids)\n",
    "tokens\n",
    "'''\n",
    "Special Token\t  [PAD]\t[UNK]\t[CLS分類]\t[SEP終止符號]\t[MASK]\n",
    "Special Token ID\t0\t100\t    101\t      102\t         103\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f07344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001667b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#多句子分詞\n",
    "string_array = [\n",
    "    string,\n",
    "    \"Baby shark, doo doo doo doo doo doo, Baby shark!\"\n",
    "]\n",
    "\n",
    "encoded_str_arr = tokenizer(string_array, padding=True, truncation=True)\n",
    "encoded_str_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b11f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "sentiment = load_dataset(\"poem_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25536028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"verse_text\"], padding=True, truncation=True)\n",
    "\n",
    "print(tokenize(sentiment[\"train\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af650d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把整個資料集都做分詞\n",
    "sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc220954",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_encoded[\"train\"][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b81fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_encoded[\"train\"][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7985a",
   "metadata": {},
   "source": [
    "# Day13-Hugging Face Transformer 入門"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_arr = [\n",
    "    \"Only those who will risk going too far can possibly find out how far one can go.\",\n",
    "    \"Baby shark, doo doo doo doo doo doo, Baby shark!\"\n",
    "]\n",
    "inputs = tokenizer(string_arr, padding=True, truncation=True, return_tensors=\"pt\") #return pytorch 的 tensor\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f59aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 Transformer Model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "'''\n",
    "第一句話的結果是 [0.8561, 0.1439]: 第0個結果的機率是0.8561，第1個結果的機率是0.1439。\n",
    "第二句話的結果是 [0.0816, 0.9184]: 第0個結果的機率是0.0816，第1個結果的機率是0.9184。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 label 來打印出來\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd10c63",
   "metadata": {},
   "source": [
    "# Day14-Hugging Face Transformer Pipeline 和 TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2691e2c",
   "metadata": {},
   "source": [
    "# Day15- Fine-tune Transformer --- 資料處理篇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "sentiment = load_dataset(\"poem_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 dataset 轉成 pandas\n",
    "import pandas as pd\n",
    "\n",
    "sentiment.set_format(type=\"pandas\")\n",
    "df = sentiment[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return sentiment[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 lebels 指定為變數\n",
    "labels = sentiment[\"train\"].features[\"label\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10158c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 dataset 的分布用 matplotlib 印出來\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Number of labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把 dataset 的格式 reset 回來\n",
    "sentiment.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#呼叫分詞\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把資料集做分詞\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"verse_text\"], padding=True, truncation=True)\n",
    "\n",
    "sentiment_encoded = sentiment.map(tokenize, batched=True, batch_size=None)\n",
    "next(iter(sentiment_encoded[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = sentiment[\"validation\"]\n",
    "valid_ds[\"label\"][:]\n",
    "''' \n",
    "dataset validate 的部份，會發現裡面都沒有類別 3 ，未來可能會在做 validation 的時候產生 bug\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238293e",
   "metadata": {},
   "source": [
    "# Day16- Fine-tune Transformer --- 訓練模型篇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入 PyTorch\n",
    "import torch\n",
    "\n",
    "#載入 pre-trained model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#使用 CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#設定 label 的數量\n",
    "num_labels = 4\n",
    "\n",
    "#指定好 id2label 和 label2id\n",
    "my_model = (AutoModelForSequenceClassification\n",
    "        .from_pretrained(model_name, num_labels=num_labels #model_name = \"distilbert-base-uncased\"\n",
    "        ,id2label={\"0\": \"negative\",\n",
    "                    \"1\": \"positive\",\n",
    "                    \"2\": \"no_impact\",\n",
    "                    \"3\": \"mixed\"}\n",
    "        ,label2id={\"negative\": \"0\",\n",
    "                    \"positive\": \"1\",\n",
    "                    \"no_impact\": \"2\",\n",
    "                    \"mixed\": \"3\"})\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(sentiment_encoded[\"train\"]) // batch_size\n",
    "model_name = \"poem_model\"\n",
    "\n",
    "#設定參數\n",
    "training_args = TrainingArguments(output_dir=model_name, # checkpoint 和最後跑完的模型儲存位置\n",
    "                                  num_train_epochs=40,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  report_to = \"azure_ml\", #Azure Machine Learning\n",
    "                                  logging_steps=logging_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練模型的期間可以監控 accuracy_score 和 f1_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#開始訓練模型\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=my_model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=sentiment_encoded[\"train\"],\n",
    "                  eval_dataset=sentiment_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa77ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用pipeline載入模型\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task= 'sentiment-analysis', \n",
    "                      model= \"poem_model/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\n",
    "    [\n",
    "        \"Only those who will risk going too far can possibly find out how far one can go.\",\n",
    "        \"Baby shark, doo doo doo doo doo doo, Baby shark!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f0b62",
   "metadata": {},
   "source": [
    "# Day18-Hugging Face 文本生成入門"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e0f68",
   "metadata": {},
   "source": [
    "###### 根據輸入的提示，產生輸出 => 條件式文本生成(conditional text generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee6391",
   "metadata": {},
   "source": [
    "###### 方法1: 得到 logits 再過 softmax 選最高機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"I have a pen, I have an \"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 10\n",
    "choices_per_step = 3\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids)\n",
    "        # 選最後一個 token 然後過 softmax 後選出機率最大\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "print(iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c63ce",
   "metadata": {},
   "source": [
    "###### 方法2: 使用 generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "input_txt = \"\"\"I have a pen, I have an iphone, I have a laptop. Thus,\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed186",
   "metadata": {},
   "source": [
    "# Day19-Hugging Face 文本生成進階"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decea7d4",
   "metadata": {},
   "source": [
    "###### 方法1: Greedy Search貪婪搜尋\n",
    "###### 缺點: 產出大量重複的字句\n",
    "###### 應用場景: 精確任務或為特定問題提供答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd073634",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "input_txt = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \\n\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that \\\n",
    "the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20ad78",
   "metadata": {},
   "source": [
    "###### 方法2: Beam Search波束搜尋\n",
    "###### 應用場景: 精確任務或為特定問題提供答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "input_txt = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \\n\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that \\\n",
    "the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=3,  do_sample=False, no_repeat_ngram_size=5)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25a460",
   "metadata": {},
   "source": [
    "###### 方法3: Sampling取樣\n",
    "###### 應用場景: 生成更長或更有創意的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "input_txt = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \\n\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that \\\n",
    "the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=1, do_sample=True, temperature=1.5)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "input_txt = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \\n\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that \\\n",
    "the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=1, do_sample=True, top_k=50)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "input_txt = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \\n\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that \\\n",
    "the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=1, do_sample=True, top_p=0.95)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa35e13",
   "metadata": {},
   "source": [
    "# Day20-Hugging Face 中文的文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e914a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,AutoModelForCausalLM\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=256\n",
    "\n",
    "input_txt = \"\"\"\n",
    "隨著貸款日益枯竭，Alistair Darling 被迫考慮對銀行進行第二次救助。 \\\n",
    "財政大臣將在幾週內決定是否向經濟中再注入數十億美元，因為有證據表明\\\n",
    "去年 370 億的部分國有化未能保持信貸流動，\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=True, top_k=50)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd51e0b",
   "metadata": {},
   "source": [
    "# Day21-Hugging Face 摘要任務入門"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93a0f0",
   "metadata": {},
   "source": [
    "###### Encoder-Decoder transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \n",
    "\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\n",
    "Mr Darling, the former Liberal Democrat chancellor, admitted that the situation had become critical but insisted that there was still time to turn things around. \n",
    "\n",
    "He told the BBC that the crisis in the banking sector was the most serious problem facing the economy but also highlighted other issues, such as the falling value of sterling and the threat of inflation. \n",
    "\n",
    "\"The worst fears about the banking crisis seem not to be panning out,\" he said, adding that there had not been a single banker arrested or charged over the crash. \n",
    "\n",
    "\"The economy, the economy\"\n",
    "\n",
    "Mr Darling said \"there's been a very, very strong recovery\" since the autumn of 2008.\n",
    "\n",
    "\"There are very big problems ahead of us, not least of which is inflation. It is likely to be a very high inflation rate. \"\n",
    "\n",
    "The economy is expected to grow by 0.3% in the quarter to the end of this year.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79eac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"t5-large\")\n",
    "result = pipe(input_text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_pegasus = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "result_pegasus = pipe_pegasus(input_text)\n",
    "result_pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbd795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"The U.S. are a country. Mr. White vs. Heisenberg.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a718982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#整理摘要結果\n",
    "paragraph_result_T5 = \"\\n\".join(sent_tokenize(result[0][\"summary_text\"]))\n",
    "print(paragraph_result_T5)\n",
    "\n",
    "print()\n",
    "\n",
    "paragraph_result_pegasus = \"\\n\".join(sent_tokenize(result_pegasus[0][\"summary_text\"].replace(\" .<n>\", \" .\\n\")))\n",
    "print(paragraph_result_pegasus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85174f91",
   "metadata": {},
   "source": [
    "# Day22-評價摘要好壞的演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f56263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_metric.compute(\n",
    "    predictions=[paragraph_result_T5], references=[input_text]\n",
    ")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d56ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_metric.compute(\n",
    "    predictions=[paragraph_result_pegasus], references=[input_text]\n",
    ")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a19b87",
   "metadata": {},
   "source": [
    "# Day23- Fine-tuned 摘要任務的 transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce201a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://huggingface.co/datasets/gopalkalpande/bbc-news-summary/raw/main/bbc-news-summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入 dataset\n",
    "from datasets import load_dataset\n",
    "remote_dataset = load_dataset(\"csv\", data_files=dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "remote_dataset.set_format(type=\"pandas\")\n",
    "\n",
    "df = remote_dataset[\"train\"][:]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0223606",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = remote_dataset.shuffle(seed=5566)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test_dataset = train_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "test_valid = train_test_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test_dataset['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949095d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"google/pegasus-cnn_dailymail\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "    input_encodings = tokenizer(dataset[\"Articles\"], max_length=512,\n",
    "                                truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(dataset[\"Summaries\"], max_length=64,\n",
    "                                     truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_pt = train_test_valid_dataset.map(convert_dataset,\n",
    "                                       batched=True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, trainer\n",
    "\n",
    "model_saved_name = model_name.split(\"/\")[-1] \n",
    "\n",
    "args = Seq2SeqTrainingArguments( \n",
    "    output_dir=f\"{model_name}-finetuned\", \n",
    "    num_train_epochs=1, \n",
    "    warmup_steps=100,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100, \n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=64,\n",
    "    report_to=\"azure_ml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d570f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1352a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92cf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aabe098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset= dataset_pt[\"train\"],\n",
    "    eval_dataset = dataset_pt[\"valid\"],\n",
    "    data_collator=seq2seq_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfff341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71a08e",
   "metadata": {},
   "source": [
    "# Day24- Hugging Face Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e14b77",
   "metadata": {},
   "source": [
    "###### Named Entity Recognition(NER)。 一般翻譯為命名實體辨識、命名實體識別，或也有人翻成專有名詞辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b44038",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \n",
    "\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\n",
    "Mr Darling, the former Liberal Democrat chancellor, admitted that the situation had become critical but insisted that there was still time to turn things around. \n",
    "\n",
    "He told the BBC that the crisis in the banking sector was the most serious problem facing the economy but also highlighted other issues, such as the falling value of sterling and the threat of inflation. \n",
    "\n",
    "\"The worst fears about the banking crisis seem not to be panning out,\" he said, adding that there had not been a single banker arrested or charged over the crash. \n",
    "\n",
    "\"The economy, the economy\"\n",
    "\n",
    "Mr Darling said \"there's been a very, very strong recovery\" since the autumn of 2008.\n",
    "\n",
    "\"There are very big problems ahead of us, not least of which is inflation. It is likely to be a very high inflation rate. \"\n",
    "\n",
    "The economy is expected to grow by 0.3% in the quarter to the end of this year.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29299d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ner = pipeline(\"ner\")\n",
    "outputs = ner(sample_text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66e1d0",
   "metadata": {},
   "source": [
    "# Day25- Hugging Face 問答任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who is Mr Darling\"\n",
    "context = \"\"\"\n",
    "Alistair Darling has been forced to consider a second bailout for banks as the lending drought worsens. \n",
    "\n",
    "The Cancellor will decide tithin weeks whether to pump billions more into the economy as evidence mounts that the 37 billion part-nationalisation last yearr has failed to keep credit flowing,\n",
    "\n",
    "Mr Darling, the former Liberal Democrat chancellor, admitted that the situation had become critical but insisted that there was still time to turn things around. \n",
    "\n",
    "He told the BBC that the crisis in the banking sector was the most serious problem facing the economy but also highlighted other issues, such as the falling value of sterling and the threat of inflation. \n",
    "\n",
    "\"The worst fears about the banking crisis seem not to be panning out,\" he said, adding that there had not been a single banker arrested or charged over the crash. \n",
    "\n",
    "\"The economy, the economy\"\n",
    "\n",
    "Mr Darling said \"there's been a very, very strong recovery\" since the autumn of 2008.\n",
    "\n",
    "\"There are very big problems ahead of us, not least of which is inflation. It is likely to be a very high inflation rate. \"\n",
    "\n",
    "The economy is expected to grow by 0.3% in the quarter to the end of this year.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ba537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59de720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe(question=question, context=context, top_k=3) #top_k=3: 會秀選出機率最高的答案前三名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the problem Mr Darling told to BBC?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe(question=question, context=context, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304e344",
   "metadata": {},
   "source": [
    "# Day27-Transformer 效能優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27bb40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce95ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1933c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 06:25:09.086600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-28 06:25:09.218734: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-28 06:25:09.906323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-28 06:25:09.906417: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-28 06:25:09.906426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7f8be233c3435ba5ce3ace730ab2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434bd887cc06435d9a1ee90ddf39fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.onnx:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bc10056ac4e61ba1b23601454aeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294b190013a548dd8a5b8d1618f18c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de2d13bc3ca4a5ea22a1c144264e4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3030b4051e47cd8bac2463c252c33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a11fa9adaa4f76952ed4c0a3c04c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35adddad4d846d9b2b3f70720b72099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b4d77795fc421f92c3ad86a9b5ef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04454e24284b4f82980fcbf3c828a852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bbb4f6882b475e94049a0fbb4c2ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f427555e0ba48688b929e8d4de6b52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9248433709144592, 'start': 11, 'end': 16, 'answer': 'Ko Ko'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(\"optimum/roberta-base-squad2\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "onnx_qa = pipeline(\"question-answering\",model=model,tokenizer=tokenizer)\n",
    "\n",
    "question = \"What's my name?\"\n",
    "context = \"My name is Ko Ko and I live in Taiwan.\"\n",
    "result = onnx_qa(question, context)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae2f0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
